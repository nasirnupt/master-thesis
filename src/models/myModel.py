import torch.nn as nn
import torch
from torch.nn.modules.transformer import TransformerDecoder, TransformerDecoderLayer
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler
class Attention(nn.Module):
    def __init__(self, hidden):
        super(Attention, self).__init__()
        self.hidden = hidden*2
        self.fc = nn.Linear(self.hidden, 1)

    def forward(self, hidden):
        # calculation of matrix M
        M = F.tanh(hidden)
        # alpha
        alpha = F.softmax(self.fc(M)).permute(0,2,1)
        att = alpha.bmm(hidden).squeeze(1)
        print(att.shape)
        return att


class Mymodel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden, padding, embedding=None, dropout=0.2):
        super(Mymodel, self).__init__()
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        # embedding layer setting
        self.embedding = nn.Embedding(vocab_size, embed_size)
        #self.embedding.weight = nn.Parameter(torch.tensor(embedding,dtype=torch.float),requires_grad=False)
        #self.embedding.weight.requires_grad = False
        self.hidden = hidden
        self.dropout = nn.Dropout(dropout)

        # TextCNN layer setting
        self.conv1 = nn.Conv2d(1,32,(2,embed_size))
        self.conv2 = nn.Conv1d(1,32,(3,embed_size))
        self.conv3 = nn.Conv1d(1,32,(4,embed_size))

        # Transfomer layer setting
        self.rnn = nn.LSTM(embed_size, hidden,bidirectional=True, batch_first=True)
    def forward(self, input1, input2):
        """
        :param input1: features generated by deep learning model
        :param input2: statistical feature from oringinal model.
        :return: output feature
        """
        embedd = self.embedding(input1)
        embedd = self.dropout(embedd)
        #print(embedd.shape)
        # textcnn for classification
        conv1 = self.conv1(embedd.unsqueeze(1))
        conv1 = F.dropout(conv1).squeeze(-1)
        conv1 = F.max_pool1d(conv1, kernel_size=conv1.size(-1)).squeeze(-1)

        conv2 = self.conv2(embedd.unsqueeze(1))
        conv2 = F.dropout(conv2).squeeze(-1)
        conv2 = F.max_pool1d(conv2, kernel_size=conv2.size(-1)).squeeze(-1)

        conv3 = self.conv3(embedd.unsqueeze(1))
        conv3 = F.dropout(conv3).squeeze(-1)
        conv3 = F.max_pool1d(conv3, kernel_size=conv3.size(-1)).squeeze(-1)

        cnn_fea = torch.cat([conv1, conv2, conv3],dim=-1)
        rnn,_ = self.rnn(embedd) #
        att = Attention(self.hidden)
        rnn_fea = att(rnn)

        # statistical feature generation:
        # 1: regularize the statistical
        # 2: concatenate with all the data
        maxsc = MinMaxScaler()
        input2 = maxsc.fit_transform(input2)
        stat_fea = torch.tensor(input2,dtype=torch.float32)
        output = torch.cat([cnn_fea,rnn_fea, stat_fea],dim=-1)

        self.linear = nn.Linear(output.shape[-1], 1)
        output = torch.sigmoid(self.linear(output)).squeeze(-1)
        return output


class CNNmodel(nn.Module):
    def __init__(self, vocab_size, embed_size, embedding, hidden, padding, dropout=0.2):
        super(CNNmodel, self).__init__()
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        # embedding layer setting
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.embedding.weight = nn.Parameter(torch.tensor(embedding,dtype=torch.float),requires_grad=False)
        self.embedding.weight.requires_grad = False
        self.hidden = hidden
        self.dropout = nn.Dropout(dropout)

        # TextCNN layer setting
        self.conv1 = nn.Conv2d(1,32,(2,embed_size))
        self.conv2 = nn.Conv1d(1,32,(3,embed_size))
        self.conv3 = nn.Conv1d(1,32,(4,embed_size))

        #self.rnn = nn.LSTM(embed_size, hidden,bidirectional=True, batch_first=True)
    def forward(self, input1, input2):
        """
        :param input1: features generated by deep learning model
        :param input2: statistical feature from oringinal model.
        :return: output feature
        """
        embedd = self.embedding(input1)
        embedd = self.dropout(embedd)
        #print(embedd.shape)
        # textcnn for classification
        conv1 = self.conv1(embedd.unsqueeze(1))
        conv1 = F.dropout(conv1).squeeze(-1)
        conv1 = F.max_pool1d(conv1, kernel_size=conv1.size(-1)).squeeze(-1)

        conv2 = self.conv2(embedd.unsqueeze(1))
        conv2 = F.dropout(conv2).squeeze(-1)
        conv2 = F.max_pool1d(conv2, kernel_size=conv2.size(-1)).squeeze(-1)

        conv3 = self.conv3(embedd.unsqueeze(1))
        conv3 = F.dropout(conv3).squeeze(-1)
        conv3 = F.max_pool1d(conv3, kernel_size=conv3.size(-1)).squeeze(-1)

        cnn_fea = torch.cat([conv1, conv2, conv3],dim=-1)
        #rnn,_ = self.rnn(embedd) #
        #att = Attention(self.hidden)
        #rnn_fea = att(rnn)

        # statistical feature generation:
        # 1: regularize the statistical
        # 2: concatenate with all the data
        maxsc = MinMaxScaler()
        input2 = maxsc.fit_transform(input2)
        stat_fea = torch.tensor(input2,dtype=torch.float32)
        output = torch.cat([cnn_fea, stat_fea],dim=-1)

        self.linear = nn.Linear(output.shape[-1], 1)
        output = torch.sigmoid(self.linear(output)).squeeze(-1)
        return output

class  BiLSTM_ATT(nn.Module):
    def __init__(self, vocab_size, embed_size, embedding, hidden, padding, dropout=0.2):
        super(BiLSTM_ATT, self).__init__()
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        # embedding layer setting
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.embedding.weight = nn.Parameter(torch.tensor(embedding,dtype=torch.float),requires_grad=False)
        self.embedding.weight.requires_grad = False
        self.hidden = hidden
        self.dropout = nn.Dropout(dropout)
        # Transfomer layer setting
        self.rnn = nn.LSTM(embed_size, hidden,bidirectional=True, batch_first=True)
        self.linear = nn.Linear(self.hidden, 1)
    def forward(self, input1):
        """
        :param input1: features generated by deep learning model
        :return: output feature
        """
        embedd = self.embedding(input1)
        embedd = self.dropout(embedd)
        #print(embedd.shape)
        # textcnn for classification
        rnn,_ = self.rnn(embedd) #
        att = Attention(self.hidden)
        rnn_fea = att(rnn)

        output = torch.sigmoid(self.linear(rnn_fea)).squeeze(-1)

        return output








